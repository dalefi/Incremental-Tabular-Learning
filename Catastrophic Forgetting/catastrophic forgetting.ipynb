{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8ae670b-c1dc-4ebf-9c01-b2b710aee317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn as skl\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from mylib import class_distributions\n",
    "from mylib import helper_funcs\n",
    "\n",
    "from mylib.pipelines import updating_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38033b5-bef3-4701-a71e-d02fab0f2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = data_folder = Path(\"../../data/DryBeanDataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5606282d-a55a-4d71-941b-f4cad0dfd2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    0.260525\n",
       "5    0.193667\n",
       "0    0.148924\n",
       "4    0.141650\n",
       "3    0.119756\n",
       "1    0.097127\n",
       "2    0.038351\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distributions.label_proportions(data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5dafb9-3c58-4a63-83d8-5dde639e7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder / 'DryBeanDataset.csv')\n",
    "\n",
    "# need feature matrix X and labels labels for xgboost\n",
    "labels = data[\"Class\"]\n",
    "X = data.drop([\"Class\"],axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29dd4432-b6ec-4538-9691-4eb633b410db",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_method = 'continued_training'\n",
    "new_class_idx = 6\n",
    "num_models = 20\n",
    "num_round = 10\n",
    "max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "629ab645-f76b-48b6-8ea8-8aea5db7da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model file \n",
    "Path('models').mkdir(parents=True, exist_ok=True)\n",
    "model_folder = Path(\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24579941-c138-47a1-9060-6d43d98818cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare smaller dataset with only first num_labels classes of beans\n",
    "old_classes = np.setdiff1d(labels.unique(), new_class_idx)\n",
    "new_class = new_class_idx\n",
    "\n",
    "# compute number of old labels used\n",
    "num_labels = len(old_classes)\n",
    "\n",
    "# relabel for XGBoost\n",
    "labels, relabel_dict = helper_funcs.relabel(labels, old_classes, new_class)\n",
    "\n",
    "# the \"original\" training data\n",
    "data_small = X[labels < num_labels]\n",
    "labels_small = labels[labels < num_labels]\n",
    "\n",
    "# to check full model on all of the old data\n",
    "dsmall = xgb.DMatrix(data_small, label=labels_small)\n",
    "\n",
    "# the new class data\n",
    "new_class_data = X[labels == num_labels]\n",
    "new_class_labels = labels[labels == num_labels]\n",
    "\n",
    "# only to check performance on the newly added data\n",
    "dnew_class = xgb.DMatrix(new_class_data, label=new_class_labels)\n",
    "\n",
    "# the entire training data\n",
    "data_full = pd.concat([data_small, new_class_data])\n",
    "labels_full = pd.concat([labels_small, new_class_labels])\n",
    "\n",
    "# to check full model on the full data\n",
    "dfull = xgb.DMatrix(data_full, label=labels_full)\n",
    "\n",
    "\n",
    "# some parameters\n",
    "proportion_of_old_data = [i*0.1 for i in range(1,10)]\n",
    "# I don't have the time to vary this\n",
    "num_round_update=[num_round]\n",
    "eta = .1\n",
    "\n",
    "# parameters for small model\n",
    "param_small = {'max_depth': max_depth,\n",
    "               'eta': eta,\n",
    "               'objective': 'multi:softprob',\n",
    "               \"num_class\": num_labels}\n",
    "param_small['nthread'] = 4\n",
    "param_small['eval_metric'] = 'mlogloss'\n",
    "\n",
    "\n",
    "# parameters for update model (the same as for full model, but just in case I want to ever change them)\n",
    "param_update = {'max_depth': max_depth,\n",
    "                'eta': eta,\n",
    "                'objective': 'multi:softprob',\n",
    "                \"num_class\": num_labels+1}\n",
    "param_update['nthread'] = 4\n",
    "param_update['eval_metric'] = 'mlogloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d48e3ea-5215-42f2-b998-dec1d97ad362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current target proportion of old data in use: 0.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m dtrain_update \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(data_update, label\u001b[38;5;241m=\u001b[39mlabels_update)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# update model\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m bst_update \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdtrain_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_round_updt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m#evals=evallist_update,\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmall_model.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m old_data_tmp[model_num] \u001b[38;5;241m=\u001b[39m skl\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(np\u001b[38;5;241m.\u001b[39margmax(bst_update\u001b[38;5;241m.\u001b[39mpredict(dsmall), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     76\u001b[0m                                                        labels_small)\n\u001b[1;32m     77\u001b[0m new_data_tmp[model_num] \u001b[38;5;241m=\u001b[39m skl\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(np\u001b[38;5;241m.\u001b[39margmax(bst_update\u001b[38;5;241m.\u001b[39mpredict(dnew_class), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     78\u001b[0m                                                        new_class_labels)\n",
      "File \u001b[0;32m~/.virtualenvs/env_master/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/env_master/lib/python3.10/site-packages/xgboost/training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    171\u001b[0m cb_container \u001b[38;5;241m=\u001b[39m CallbackContainer(\n\u001b[1;32m    172\u001b[0m     callbacks,\n\u001b[1;32m    173\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     output_margin\u001b[38;5;241m=\u001b[39mcallable(obj) \u001b[38;5;129;01mor\u001b[39;00m metric_fn \u001b[38;5;129;01mis\u001b[39;00m feval,\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    180\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mbefore_training(bst)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# these dictionaries are filled with the results and later pickled\n",
    "\n",
    "old_data_mean_results = dict()\n",
    "old_data_std_results = dict()\n",
    "new_data_mean_results = dict()\n",
    "new_data_std_results = dict()\n",
    "update_data_mean_results = dict()\n",
    "update_data_std_results = dict()\n",
    "full_data_mean_results = dict()\n",
    "full_data_std_results = dict()\n",
    "\n",
    "# the update routine\n",
    "for num_round_update_idx, num_round_updt in enumerate(num_round_update):\n",
    "\n",
    "    # initialize arrays where results are stored\n",
    "    old_data_mean = np.zeros(len(proportion_of_old_data))\n",
    "    old_data_std = np.zeros(len(proportion_of_old_data))\n",
    "    new_data_mean = np.zeros(len(proportion_of_old_data))\n",
    "    new_data_std = np.zeros(len(proportion_of_old_data))\n",
    "    update_data_mean = np.zeros(len(proportion_of_old_data))\n",
    "    update_data_std = np.zeros(len(proportion_of_old_data))\n",
    "    full_data_mean = np.zeros(len(proportion_of_old_data))\n",
    "    full_data_std = np.zeros(len(proportion_of_old_data))\n",
    "\n",
    "    for proportion_num, proportion in enumerate(proportion_of_old_data):\n",
    "        print(f\"Current target proportion of old data in use: {proportion}\")\n",
    "\n",
    "        # initialize arrays where temporary results are stored\n",
    "        old_data_tmp = np.zeros(num_models)\n",
    "        new_data_tmp = np.zeros(num_models)\n",
    "        update_data_tmp = np.zeros(num_models)\n",
    "        full_data_tmp = np.zeros(num_models)\n",
    "\n",
    "        for model_num in range(num_models):\n",
    "            \n",
    "            # training the original model\n",
    "            \n",
    "            seed = np.random.randint(0,100)\n",
    "            # split original data into train- and test-data\n",
    "            X_train_small, X_test_small, y_train_small, y_test_small = skl.model_selection.train_test_split(data_small, \n",
    "                                                                                                            labels_small,\n",
    "                                                                                                            test_size=.2,\n",
    "                                                                                                            random_state=seed)\n",
    "\n",
    "            # specify DMatrices\n",
    "            dtrain_small = xgb.DMatrix(X_train_small, label=y_train_small)\n",
    "            dtest_small = xgb.DMatrix(X_test_small, label=y_test_small)\n",
    "            \n",
    "            evallist_small = [(dtrain_small, 'train'), (dtest_small, 'eval')]\n",
    "            \n",
    "            bst_small = xgb.train(param_small,\n",
    "                                  dtrain_small,\n",
    "                                  num_round,\n",
    "                                  evals=evallist_small,\n",
    "                                  verbose_eval=False)\n",
    "\n",
    "            bst_small.save_model(fname=model_folder / 'small_model.json')\n",
    "                \n",
    "            # concatenate selected data with data of new class\n",
    "            data_update = new_class_data\n",
    "            labels_update = new_class_labels\n",
    "\n",
    "            # use all the update data to update the model\n",
    "            dtrain_update = xgb.DMatrix(data_update, label=labels_update)\n",
    "            \n",
    "            # update model\n",
    "            bst_update = xgb.train(param_update,\n",
    "                                  dtrain_update,\n",
    "                                  num_round_updt,\n",
    "                                  verbose_eval=False,\n",
    "                                  xgb_model=model_folder/\"small_model.json\")\n",
    "\n",
    "            \n",
    "            old_data_tmp[model_num] = skl.metrics.accuracy_score(np.argmax(bst_update.predict(dsmall), axis=1),\n",
    "                                                                   labels_small)\n",
    "            new_data_tmp[model_num] = skl.metrics.accuracy_score(np.argmax(bst_update.predict(dnew_class), axis=1),\n",
    "                                                                   new_class_labels)\n",
    "            update_data_tmp[model_num] = skl.metrics.accuracy_score(np.argmax(bst_update.predict(dtrain_update), axis=1),\n",
    "                                                                     labels_update)\n",
    "            full_data_tmp[model_num] = skl.metrics.accuracy_score(np.argmax(bst_update.predict(dfull), axis=1),\n",
    "                                                                    labels_full)\n",
    "\n",
    "        old_data_mean[proportion_num] = old_data_tmp.mean()\n",
    "        old_data_std[proportion_num] = old_data_tmp.std()\n",
    "        new_data_mean[proportion_num] = new_data_tmp.mean()\n",
    "        new_data_std[proportion_num] = new_data_tmp.std()  \n",
    "        update_data_mean[proportion_num] = update_data_tmp.mean()\n",
    "        update_data_std[proportion_num] = update_data_tmp.std()\n",
    "        full_data_mean[proportion_num] = full_data_tmp.mean()\n",
    "        full_data_std[proportion_num] = full_data_tmp.std()\n",
    "\n",
    "    old_data_mean_results[num_round_updt] = old_data_mean\n",
    "    old_data_std_results[num_round_updt] = old_data_std\n",
    "    new_data_mean_results[num_round_updt] = new_data_mean\n",
    "    new_data_std_results[num_round_updt] = new_data_std\n",
    "    update_data_mean_results[num_round_updt] = update_data_mean\n",
    "    update_data_std_results[num_round_updt] = update_data_std\n",
    "    full_data_mean_results[num_round_updt] = full_data_mean\n",
    "    full_data_std_results[num_round_updt] = full_data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67be5c2-b960-4c29-90a3-021c274f341e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_master",
   "language": "python",
   "name": "env_master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
