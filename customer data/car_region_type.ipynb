{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057cf19b-dcde-4675-a1f5-709bc107cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import sklearn as skl\n",
    "import _pickle as pickle\n",
    "\n",
    "from mylib import class_distributions\n",
    "from mylib import data_selection\n",
    "from mylib import helper_funcs\n",
    "from mylib.db import preprocessing\n",
    "from mylib.db import constants\n",
    "\n",
    "import dtreeviz\n",
    "import logging\n",
    "# to suppress messages when plotting trees\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(level=logging.CRITICAL)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d48048c-12d2-4133-a4ee-4c4aa39baaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing successful.\n"
     ]
    }
   ],
   "source": [
    "from_date = \"2022-06-01\"\n",
    "to_date = \"2022-06-30\"\n",
    "n = 100000\n",
    "\n",
    "car_df_unprocessed = preprocessing.Preprocessor(from_date, to_date, limit=n, verbose=False,\n",
    "                                               remove_cat_columns = False, normalization = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4452101d-3979-4715-8022-e2e9161327ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing column 'age_segment'\n",
      "Removing column 'region_type'\n",
      "Removing column 'emp_liable'\n",
      "Removing column 'hou_fam_structure'\n",
      "Removing column 'hou_aff_new_products'\n",
      "Removing column 'hou_aff_prices'\n",
      "Removing column 'subs_hand_ind'\n",
      "Removing column 'bnt_vvl_lng'\n",
      "Removing column 'article_status_enc'\n",
      "Removing column 'tech_generation'\n",
      "Removing column 'known_article'\n",
      "Removing column 'equal_to_recently_sold'\n",
      "(99689, 151)\n",
      "(99689,)\n"
     ]
    }
   ],
   "source": [
    "# first capture the column to be predicted\n",
    "labels = car_df_unprocessed.car_df[\"region_type\"]\n",
    "\n",
    "# now remove all categorical columns and normalize\n",
    "car_df_unprocessed.remove_columns(constants.CATEGORICAL_COLUMNS)\n",
    "car_df_unprocessed.normalize()\n",
    "X = car_df_unprocessed.car_df\n",
    "\n",
    "print(X.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cafca2e5-4e13-47e1-979e-c4d092c1309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to drop unknown or missing values\n",
    "nan_index = labels[np.isnan(labels)].index\n",
    "unknown_index = labels[labels == -1].index\n",
    "\n",
    "labels = labels.drop(nan_index)\n",
    "labels = labels.drop(unknown_index)\n",
    "X = X.drop(nan_index)\n",
    "X = X.drop(unknown_index)\n",
    "\n",
    "assert (labels.index == X.index).all()\n",
    "\n",
    "labels = labels-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59700ee4-5967-4f11-8d4c-437de17a54ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.367682\n",
       "1    0.334212\n",
       "0    0.298106\n",
       "Name: region_type, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distributions.label_proportions(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c2141f-39a1-4eba-8e53-d3a7d811df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare smaller dataset with only subset of classes\n",
    "\n",
    "old_classes = [0,2]\n",
    "new_class = 1\n",
    "\n",
    "# compute number of old labels used\n",
    "num_labels = len(old_classes)\n",
    "\n",
    "# relabel for XGBoost\n",
    "labels = helper_funcs.relabel(labels, old_classes, new_class)\n",
    "\n",
    "data_small = X[labels < num_labels]\n",
    "labels_small = labels[labels < num_labels]\n",
    "\n",
    "# attempt to retrain with new data\n",
    "data_update = X[labels == num_labels]\n",
    "labels_update = labels[labels == num_labels]\n",
    "\n",
    "# also train a model with all the data availale for comparison\n",
    "data_full = pd.concat([data_small, data_update])\n",
    "labels_full = pd.concat([labels_small, labels_update])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6404c63-cb71-4c7d-b74d-487a4b6612de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train- and test-data\n",
    "\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = skl.model_selection.train_test_split(data_small, \n",
    "                                                    labels_small,\n",
    "                                                    test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89f1bfc6-5a30-410c-9e80-269f4524e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify DMatrices\n",
    "\n",
    "dtrain_small = xgb.DMatrix(X_train_small, label=y_train_small)\n",
    "dtest_small = xgb.DMatrix(X_test_small, label=y_test_small)\n",
    "\n",
    "# specify some parameters\n",
    "proportion_of_old_data = [i*0.1 for i in range(1,10)]\n",
    "num_models = 1\n",
    "\n",
    "# specify paramters for XGBoost\n",
    "num_round = 100\n",
    "num_round_full = 200\n",
    "early_stopping_rounds = num_round*.1\n",
    "max_depth = 3\n",
    "eta = .1\n",
    "\n",
    "param_small = {'max_depth': max_depth, 'eta': eta, 'objective': 'multi:softprob', \"num_class\": num_labels}\n",
    "param_small['nthread'] = 4\n",
    "param_small['eval_metric'] = 'mlogloss'\n",
    "\n",
    "evallist_small = [(dtrain_small, 'train'), (dtest_small, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffc10f4-21f5-425e-b593-8d2687cf85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.61290\teval-mlogloss:0.61315\n",
      "[50]\ttrain-mlogloss:0.07094\teval-mlogloss:0.07193\n",
      "[99]\ttrain-mlogloss:0.03932\teval-mlogloss:0.04063\n"
     ]
    }
   ],
   "source": [
    "# training model with fewer labels\n",
    "bst_small = xgb.train(param_small,\n",
    "                      dtrain_small,\n",
    "                      num_round,\n",
    "                      evals=evallist_small,\n",
    "                      #early_stopping_rounds=early_stopping_rounds,\n",
    "                      verbose_eval=50)\n",
    "\n",
    "bst_small.save_model('small_model_region_type.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d1a4c5-d6d0-4a32-b8c0-6713487193fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.9873469723112316\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data: \", skl.metrics.accuracy_score(np.argmax(bst_small.predict(dtest_small), axis=1), y_test_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adc973c-e0d1-45b9-a4e3-1c2c416c35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train- and test-data\n",
    "\n",
    "X_train_update, X_test_update, y_train_update, y_test_update = skl.model_selection.train_test_split(data_update,\n",
    "                                                                                                    labels_update,\n",
    "                                                                                                    test_size=.2)\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = skl.model_selection.train_test_split(data_full,\n",
    "                                                                                            labels_full,\n",
    "                                                                                            test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9be07b2-829a-432b-93b8-ae9c3f9f4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify DMatrices\n",
    "\n",
    "dtrain_update = xgb.DMatrix(X_train_update, label=y_train_update)\n",
    "dtest_update = xgb.DMatrix(X_test_update, label=y_test_update)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest_full = xgb.DMatrix(X_test_full, label=y_test_full)\n",
    "\n",
    "\n",
    "# specify paramters for XGBoost\n",
    "param_update = {'max_depth': max_depth,\n",
    "                'eta': eta,\n",
    "                'objective': 'multi:softprob',\n",
    "                \"num_class\": num_labels+1}\n",
    "param_update['nthread'] = 4\n",
    "param_update['eval_metric'] = 'mlogloss'\n",
    "\n",
    "evallist_update = [(dtrain_update, 'train'), (dtest_update, 'eval')]\n",
    "\n",
    "\n",
    "param_full = {'max_depth': max_depth,\n",
    "              'eta': eta,\n",
    "              'objective': 'multi:softprob',\n",
    "              \"num_class\": num_labels+1}\n",
    "param_full['nthread'] = 4\n",
    "param_full['eval_metric'] = 'mlogloss'\n",
    "\n",
    "evallist_full = [(dtrain_full, 'train'), (dtest_full, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927c3320-bbf7-4c44-93c8-9cc1f3f2a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.00713\teval-mlogloss:1.00696\n",
      "[25]\ttrain-mlogloss:0.40138\teval-mlogloss:0.40250\n",
      "[50]\ttrain-mlogloss:0.31069\teval-mlogloss:0.31309\n",
      "[75]\ttrain-mlogloss:0.27065\teval-mlogloss:0.27278\n",
      "[100]\ttrain-mlogloss:0.24724\teval-mlogloss:0.24954\n",
      "[125]\ttrain-mlogloss:0.23014\teval-mlogloss:0.23272\n",
      "[150]\ttrain-mlogloss:0.21517\teval-mlogloss:0.21845\n",
      "[175]\ttrain-mlogloss:0.20159\teval-mlogloss:0.20509\n",
      "[199]\ttrain-mlogloss:0.18928\teval-mlogloss:0.19304\n"
     ]
    }
   ],
   "source": [
    "# training a model with all the training data\n",
    "\n",
    "bst_full = xgb.train(param_full,\n",
    "                     dtrain_full,\n",
    "                     num_round_full,\n",
    "                     evals=evallist_full,\n",
    "                     #early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6add0ab8-6a9d-498b-a721-967c0db73af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.9275164113785558\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data: \", skl.metrics.accuracy_score(np.argmax(bst_full.predict(dtest_full), axis=1), y_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a5448b-bcde-4d50-b7ce-63ed71db6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current target proportion of old data in use: 0.1\n",
      "Current target proportion of old data in use: 0.2\n",
      "Current target proportion of old data in use: 0.30000000000000004\n",
      "Current target proportion of old data in use: 0.4\n",
      "Current target proportion of old data in use: 0.5\n",
      "Current target proportion of old data in use: 0.6000000000000001\n",
      "Current target proportion of old data in use: 0.7000000000000001\n",
      "Current target proportion of old data in use: 0.8\n",
      "Current target proportion of old data in use: 0.9\n"
     ]
    }
   ],
   "source": [
    "random_old = []\n",
    "random_new = []\n",
    "random_mixed = []\n",
    "random_full = []\n",
    "\n",
    "\n",
    "for proportion in proportion_of_old_data:\n",
    "    print(f\"Current target proportion of old data in use: {proportion}\")\n",
    "    \n",
    "    random_old_tmp = 0\n",
    "    random_new_tmp = 0\n",
    "    random_mixed_tmp = 0\n",
    "    random_full_tmp = 0\n",
    "\n",
    "    for _ in range(num_models):\n",
    "\n",
    "        _, old_data_part, _, old_y_part = skl.model_selection.train_test_split(data_small,\n",
    "                                                                                labels_small,\n",
    "                                                                                test_size=proportion)\n",
    "        \n",
    "\n",
    "        data_update2 = pd.concat([old_data_part, data_update])\n",
    "        labels_update2 = pd.concat([old_y_part, labels_update])\n",
    "\n",
    "        X_train_update2, X_test_update2, y_train_update2, y_test_update2 = skl.model_selection.train_test_split(data_update2,\n",
    "                                                                                                                labels_update2,\n",
    "                                                                                                                test_size=.2)\n",
    "\n",
    "        # create DMatrices\n",
    "\n",
    "        dtrain_update2 = xgb.DMatrix(X_train_update2, label=y_train_update2)\n",
    "        dtest_update2 = xgb.DMatrix(X_test_update2, label=y_test_update2)\n",
    "\n",
    "        # train model\n",
    "        bst_update2 = xgb.train(param_update,\n",
    "                                  dtrain_update2,\n",
    "                                  num_round,\n",
    "                                  evals=evallist_update,\n",
    "                                  early_stopping_rounds=early_stopping_rounds,\n",
    "                                  verbose_eval=False,\n",
    "                                  xgb_model=\"small_model_region_type.json\")\n",
    "\n",
    "        random_old_tmp += skl.metrics.accuracy_score(np.argmax(bst_update2.predict(dtest_small), axis=1), y_test_small)\n",
    "        random_new_tmp += skl.metrics.accuracy_score(np.argmax(bst_update2.predict(dtest_update), axis=1), y_test_update)\n",
    "        random_mixed_tmp += skl.metrics.accuracy_score(np.argmax(bst_update2.predict(dtest_update2), axis=1), y_test_update2)\n",
    "        random_full_tmp += skl.metrics.accuracy_score(np.argmax(bst_update2.predict(dtest_full), axis=1), y_test_full)\n",
    "        \n",
    "    random_old.append(random_old_tmp/num_models)\n",
    "    random_new.append(random_new_tmp/num_models)    \n",
    "    random_mixed.append(random_mixed_tmp/num_models)    \n",
    "    random_full.append(random_full_tmp/num_models)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee634d9-299c-4e59-8805-417da161a2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current target proportion of old data in use: 0.1\n",
      "Current target proportion of old data in use: 0.2\n",
      "Current target proportion of old data in use: 0.30000000000000004\n",
      "Current target proportion of old data in use: 0.4\n",
      "Current target proportion of old data in use: 0.5\n",
      "Current target proportion of old data in use: 0.6000000000000001\n",
      "Current target proportion of old data in use: 0.7000000000000001\n",
      "Current target proportion of old data in use: 0.8\n",
      "Current target proportion of old data in use: 0.9\n",
      "Current target proportion of old data in use: 0.1\n",
      "Current target proportion of old data in use: 0.2\n",
      "Current target proportion of old data in use: 0.30000000000000004\n",
      "Current target proportion of old data in use: 0.4\n",
      "Current target proportion of old data in use: 0.5\n",
      "Current target proportion of old data in use: 0.6000000000000001\n",
      "Current target proportion of old data in use: 0.7000000000000001\n",
      "Current target proportion of old data in use: 0.8\n",
      "Current target proportion of old data in use: 0.9\n",
      "Current target proportion of old data in use: 0.1\n",
      "Current target proportion of old data in use: 0.2\n",
      "Current target proportion of old data in use: 0.30000000000000004\n",
      "Current target proportion of old data in use: 0.4\n",
      "Current target proportion of old data in use: 0.5\n",
      "Current target proportion of old data in use: 0.6000000000000001\n",
      "Current target proportion of old data in use: 0.7000000000000001\n",
      "Current target proportion of old data in use: 0.8\n",
      "Current target proportion of old data in use: 0.9\n",
      "Current target proportion of old data in use: 0.1\n",
      "Current target proportion of old data in use: 0.2\n",
      "Current target proportion of old data in use: 0.30000000000000004\n",
      "Current target proportion of old data in use: 0.4\n",
      "Current target proportion of old data in use: 0.5\n",
      "Current target proportion of old data in use: 0.6000000000000001\n",
      "Current target proportion of old data in use: 0.7000000000000001\n",
      "Current target proportion of old data in use: 0.8\n",
      "Current target proportion of old data in use: 0.9\n",
      "Current target proportion of old data in use: 0.1\n",
      "Current target proportion of old data in use: 0.2\n",
      "Current target proportion of old data in use: 0.30000000000000004\n",
      "Current target proportion of old data in use: 0.4\n",
      "Current target proportion of old data in use: 0.5\n",
      "Current target proportion of old data in use: 0.6000000000000001\n",
      "Current target proportion of old data in use: 0.7000000000000001\n",
      "Current target proportion of old data in use: 0.8\n",
      "Current target proportion of old data in use: 0.9\n"
     ]
    }
   ],
   "source": [
    "critical_old_alphas = {}\n",
    "critical_new_alphas = {}\n",
    "critical_mixed_alphas = {}\n",
    "critical_full_alphas = {}\n",
    "\n",
    "for alpha in [.2*i for i in range(5)]:\n",
    "\n",
    "    critical_old = []\n",
    "    critical_new = []\n",
    "    critical_mixed = []\n",
    "    critical_full = []\n",
    "\n",
    "    for proportion in proportion_of_old_data:\n",
    "        print(f\"Current target proportion of old data in use: {proportion}\")\n",
    "\n",
    "        # get critical data\n",
    "        critical_data, critical_data_labels = data_selection.get_samples_nearest_neighbors(data_small,\n",
    "                                                                                              labels_small,\n",
    "                                                                                              data_update,\n",
    "                                                                                              ratio_return_total = proportion,\n",
    "                                                                                              normalization=\"min_max\",\n",
    "                                                                                              alpha=alpha,\n",
    "                                                                                              remove_duplicates=False)\n",
    "\n",
    "\n",
    "\n",
    "        # concatenate with data for new class\n",
    "        critical_data = pd.concat([critical_data, data_update])\n",
    "        critical_data_labels = pd.concat([critical_data_labels, labels_update])\n",
    "\n",
    "        # train a model with the new class and the critical data\n",
    "        critical_old_tmp = 0\n",
    "        critical_new_tmp = 0\n",
    "        critical_mixed_tmp = 0\n",
    "        critical_full_tmp = 0\n",
    "\n",
    "        for i in range(num_models):\n",
    "            X_train_critical, X_test_critical, y_train_critical, y_test_critical = skl.model_selection.train_test_split(critical_data,\n",
    "                                                                                                                        critical_data_labels,\n",
    "                                                                                                                        test_size=.2)\n",
    "\n",
    "            dtrain_critical = xgb.DMatrix(X_train_critical, label=y_train_critical)\n",
    "            dtest_critical = xgb.DMatrix(X_test_critical, label=y_test_critical)\n",
    "\n",
    "            # updating the model with the new class\n",
    "            bst_critical = xgb.train(param_update,\n",
    "                                      dtrain_critical,\n",
    "                                      num_round,\n",
    "                                      evals=evallist_update,\n",
    "                                      early_stopping_rounds=early_stopping_rounds,\n",
    "                                      verbose_eval=False,\n",
    "                                      xgb_model=\"small_model_region_type.json\")\n",
    "\n",
    "            critical_old_tmp += skl.metrics.accuracy_score(np.argmax(bst_critical.predict(dtest_small), axis=1), y_test_small)\n",
    "            critical_new_tmp += skl.metrics.accuracy_score(np.argmax(bst_critical.predict(dtest_update), axis=1), y_test_update)\n",
    "            critical_mixed_tmp += skl.metrics.accuracy_score(np.argmax(bst_critical.predict(dtest_critical), axis=1), y_test_critical)\n",
    "            critical_full_tmp += skl.metrics.accuracy_score(np.argmax(bst_critical.predict(dtest_full), axis=1), y_test_full)\n",
    "\n",
    "        critical_old.append(critical_old_tmp/num_models)\n",
    "        critical_new.append(critical_new_tmp/num_models)\n",
    "        critical_mixed.append(critical_mixed_tmp/num_models)\n",
    "        critical_full.append(critical_full_tmp/num_models)\n",
    "    \n",
    "    critical_old_alphas[f\"{alpha}\"] = critical_old\n",
    "    critical_new_alphas[f\"{alpha}\"] = critical_new\n",
    "    critical_mixed_alphas[f\"{alpha}\"] = critical_mixed\n",
    "    critical_full_alphas[f\"{alpha}\"] = critical_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20258f06-7bd8-43d8-a173-75e53bd019e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save the performances\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m      3\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n\u001b[1;32m      4\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlim([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# save the performances\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0.8, 1])\n",
    "plt.title(f\"NearestNeighbors, minmax, including duplicates, train middle class\")\n",
    "plt.plot(proportion_of_old_data, random_full, label=\"model updated with random data\")\n",
    "for key in critical_full_alphas.keys():\n",
    "    plt.plot(proportion_of_old_data, critical_full_alphas[key], label=f\"model updated with critical data (alpha={key:.3})\")\n",
    "    \n",
    "plt.axhline(skl.metrics.accuracy_score(np.argmax(bst_full.predict(dtest_full), axis=1), y_test_full),\n",
    "            color = \"black\",\n",
    "            linestyle = \"--\",\n",
    "            label = \"batch training on full data\")\n",
    "plt.xlabel(\"Percentage of old data used in updating\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(f\"NN, minmax, including duplicates, train middle class.png\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0519f9-1fa9-4b0d-8e78-35ac7bf04ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Ergebnisse lagen alle unterhalb der zufällig ausgewählten Daten. Am besten schnitt alpha=0.8 ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226dbc2b-49af-4f85-822b-d04ec69d5419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
